---
title: "Compute metric significance"
output: html_notebook
params:
  input_metrics_file_prefix: "results/3779c75e/metrics"
  background_type: "ref"
  random_seed: 42
  significance_threshold: 0.05
---

# Setup

```{r message=FALSE}
library(magrittr)
library(tidyverse)
library(glue)
library(arrow)
library(matric)
library(logger)
source("utils.R")
source("retrieval_baseline.R")
```


```{r}
set.seed(params$random_seed)
```


```{r}
future::plan(future::multisession, workers = 14)
```


```{r}
knitr::opts_chunk$set(fig.height = 8, fig.width = 8, rows.print = 20)
```


```{r}
cat(yaml::as.yaml(params))
```


```{r}
type <- params$background_type
```

# Load metrics


```{r}
metric_set_names <-
  c(glue("level_1_0_{type}"),
    glue("level_1_{type}"),
    glue("level_2_1_{type}"))


metric_sets <-
  map(metric_set_names, function(metric_set) {
    parquet_file <-
      with(params,
           glue("{input_metrics_file_prefix}_{metric_set}.parquet"))


    if (file.exists(glue(parquet_file))) {
      log_info("Reading {parquet_file} ...")
      return(arrow::read_parquet(glue(parquet_file)))

    } else
    {
      log_info("Does not exist, skipping: {parquet_file} ...")

      return(NULL)
    }
  })

names(metric_sets) <- paste(metric_set_names, "metrics", sep = "_")

all_same_cols_rep <- attr(metric_sets[[1]], "all_same_cols_rep")

all_same_cols_group <-
  attr(metric_sets[[2]], "all_same_cols_group")
```


# Process metrics

## Level 1_0

```{r}
level_1_0_metrics <-
  metric_sets[[glue("level_1_0_{type}_metrics")]]
```


```{r}
cat(yaml::as.yaml(attr(level_1_0_metrics, "params")))
```

## Level 1

After reading level_1, drop duplicates that may result from annotating level 1_0 entities

```{r}
level_1_metrics <-
  metric_sets[[glue("level_1_{type}_metrics")]] %>%
  select(all_of(all_same_cols_rep), matches("^sim_")) %>%
  distinct()
```


```{r}
cat(yaml::as.yaml(attr(level_1_metrics, "params")))
```

## Level 2_1

After reading level_2_1, drop duplicates that may result from annotating level 1_0 entities

```{r}
level_2_1_metrics <-
  metric_sets[[glue("level_2_1_{type}_metrics")]]

if (!is.null(level_2_1_metrics)) {
  level_2_1_metrics <-
    level_2_1_metrics %>%
    select(all_of(all_same_cols_group), matches("^sim_")) %>%
    distinct()

}
```


```{r}
cat(yaml::as.yaml(attr(level_1_metrics, "params")))
```

## Compute null thresholds

```{r}
level_1_0_metrics <-
  retrieval_baseline(
    level_1_0_metrics,
    significance_threshold = params$significance_threshold,
    background_type = params$background_type,
    level_identifier = "i",
    random_seed = params$random_seed
  )
```

## Compute p-values and adjust metrics

### Level 1_0

```{r}
sim_retrieval_average_precision_type_i_nlog10pvalue <-
  glue("sim_retrieval_average_precision_{type}_i_nlog10pvalue")

sim_retrieval_average_precision_type_i <-
  glue("sim_retrieval_average_precision_{type}_i")


sim_retrieval_average_precision_type_i_adjusted <-
  glue("sim_retrieval_average_precision_{type}_i_adjusted")

sim_retrieval_r_precision_type_i_adjusted <-
  glue("sim_retrieval_r_precision_{type}_i_adjusted")


level_1_0_metrics_null_adjusted <-
  level_1_0_metrics %>%
  rowwise() %>%
  mutate("{sim_retrieval_average_precision_type_i_nlog10pvalue}" :=
    -log10((
      1 + sum(
        sim_stat_average_precision_null_samples$sim_stat_average_precision_null_samples >
          .data[[sim_retrieval_average_precision_type_i]]
      )
    ) /
      (
        1 + nrow(sim_stat_average_precision_null_samples)
      ))) %>%
  ungroup() %>%
  select(-sim_stat_average_precision_null_samples) %>%
  mutate(
    "{sim_retrieval_average_precision_type_i_adjusted}" :=
      .data[[glue("sim_retrieval_average_precision_{type}_i")]] - sim_stat_average_precision_null,
    "{sim_retrieval_r_precision_type_i_adjusted}" :=
      .data[[glue("sim_retrieval_r_precision_{type}_i")]] - sim_stat_r_precision_null
  )

c("all_same_cols_rep", "metric_metadata", "params") %>%
  walk(function(a) {
    attr(level_1_0_metrics_null_adjusted, a) <<-
      attr(level_1_0_metrics, a)
  })
```


## Aggregate metrics

### Level 1

```{r}
summary_cols <- attr(level_1_0_metrics, "all_same_cols_rep")

annotation_cols <- attr(level_1_0_metrics, "params")$calculate_index$sim_params$annotation_cols

annotation_cols_full <- unique(c(summary_cols, annotation_cols))

metadata <-
  level_1_0_metrics %>%
  dplyr::distinct(across(all_of(annotation_cols_full)))
```


After creating level_1, drop duplicates that may result from annotating level 1_0 entities

```{r}
level_1_metrics_null_adjusted <-
  level_1_0_metrics_null_adjusted %>%
  ungroup() %>%
  group_by(dplyr::across(dplyr::all_of(summary_cols))) %>%
  summarise(
    across(
      starts_with("sim_"),
      list(mean_i = mean, median_i = median)
    ),
    .groups = "keep"
  ) %>%
  dplyr::inner_join(metadata, by = summary_cols) %>%
  dplyr::select(all_of(annotation_cols_full), dplyr::everything()) %>%
  ungroup()

level_1_metrics_null_adjusted <-
  level_1_metrics_null_adjusted %>%
  select(all_of(all_same_cols_rep), matches("^sim_")) %>%
  distinct()

c("all_same_cols_rep", "metric_metadata", "params") %>%
  walk(function(a) {
    attr(level_1_metrics_null_adjusted, a) <<-
      attr(level_1_metrics, a)
  })
```


```{r}
stopifnot(
  compare::compare(
    level_1_metrics_null_adjusted %>%
      select(all_of(names(level_1_metrics))),
    level_1_metrics,
    ignoreAttrs = TRUE
  )$result
)
```

# Correct for multiple testing

## Level 1_0

I'm not sure what's the right way of correcting at Level 1_0, where there are groups of highly correlated hypothesis (each set of replicates is a correlated hypothesis set)

## Level 1

```{r}
sim_retrieval_average_precision_type_i_nlog10qvalue_mean_i <-
  glue("sim_retrieval_average_precision_{type}_i_nlog10qvalue_mean_i")

sim_retrieval_average_precision_type_i_nlog10pvalue_mean_i <-
  glue("sim_retrieval_average_precision_{type}_i_nlog10pvalue_mean_i")

level_1_metrics_null_adjusted <-
  level_1_metrics_null_adjusted %>%
  mutate(
    "{sim_retrieval_average_precision_type_i_nlog10qvalue_mean_i}" :=
      -log10(p.adjust(10**-.data[[sim_retrieval_average_precision_type_i_nlog10pvalue_mean_i]],
        method = "BH"
      ))
  )
```


# Write

```{r}
metric_set <- glue("level_1_0_{type}_null_adjusted")

parquet_file <-
  with(
    params,
    glue("{input_metrics_file_prefix}_{metric_set}.parquet")
  )

log_info("Writing {parquet_file} ...")

attr(level_1_0_metrics_null_adjusted, "significance_threshold") <- params$significance_threshold

level_1_0_metrics_null_adjusted %>%
  arrow::write_parquet(glue(parquet_file))
```


```{r}
metric_set <- glue("level_1_{type}_null_adjusted")

parquet_file <-
  with(
    params,
    glue("{input_metrics_file_prefix}_{metric_set}.parquet")
  )

log_info("Writing {parquet_file} ...")

level_1_metrics_null_adjusted %>%
  arrow::write_parquet(glue(parquet_file))
```

# Checks


```{r}
profiles <-
  arrow::read_parquet(attributes(level_1_0_metrics)$params$calculate_metrics$input_profile_file)
```

```{r}
metrics_counts <-
  level_1_metrics_null_adjusted %>%
  count(across(all_of(c(glue("sim_stat_signal_n_{type}_i_mean_i"))))) %>%
  mutate(n_perts = .data[[glue("sim_stat_signal_n_{type}_i_mean_i")]] + 1) %>%
  select(n_perts, n_groups = n) %>%
  arrange(n_perts)

metrics_counts
```


```{r}
if (!is.null(attributes(level_1_0_metrics)$params$prepare_data$split_rows_on_column)) {
  compact_splits <- attributes(level_1_0_metrics)$params$prepare_data$compact_splits

  split_col <-
    str_c(
      attributes(level_1_0_metrics)$params$prepare_data$split_rows_on_column,
      ifelse(compact_splits, "_split_compact", "_split")
    )

  profiles_counts <-
    profiles %>%
    filter(Metadata_reference_or_other != "reference") %>%
    count(across(all_of(c(split_col))), name = "n_perts") %>%
    count(n_perts, name = "n_groups") %>%
    filter(n_perts > 1) %>%
    arrange(n_perts)

  stopifnot(compare::compare(metrics_counts, profiles_counts, ignoreAttrs = TRUE)$result)
}
```

